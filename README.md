# k8s-basics
Cheatsheet and basic concepts about Kubernetes (Including KOPS and Helm)

## Kubectl

Install kubectl at Amazon Linux2 AMI (RedHat based): https://github.com/pulpbill/daily-bash-scripts/blob/master/kubectl.sh

Create env from file:
```
kubectl create -f myfile.yaml
```
Get info:
```
kubectl get <object>
```  
Get detailed info about an object:
```
kubectl describe <object>
```
Check pod logs (IE: to check actual nginx log files inside our container=pod):
```
kubectl logs <pod-ID> -f 
```
Connect to the shell’s container:
```
kubectl exec -it <pod-ID> bash OR -- sh
```
Update our environment and add more pods:

-Edit our yaml file where we described the deployment and set the value “replicas:” to the numbers of pods (containers) we want.

-Run:
```
kubectl apply -f myfile.yaml
```
Delete (terminate) a pod:
```
kubectl delete pod <pod-ID>
```
Edit an object:
```
kubectl edit pod/<pod-ID>
```
If I create an object like this:
```
kubectl create -f myfile.yaml
```
And inside of the yaml file I defined the kind of objects, it will be generated by itself, instead of doing : kubectl create deployment or some other service and could also add a service and its exposed port, instead of (after creating the deploy) doing a kubectl expose deployment etc.

Create a configmap:
```
kubectl create configmap nginx-content --from-file=<my-folder>
```
Get info of a configmap:
```
kubectl describe cm <configmap-name>
```
Delete:
```
kubectl delete cm <configmap-name>
```

Advanced commands:
```
kubectl get nodes -o json |
       jq ".items[] | {name:.metadata.name} + .status.capacity"
```
```
IP=$(kubectl get svc elastic -o go-template --template '{{ .spec.clusterIP }}')
```
```
NODEPORT=$(kubectl get svc/registry -o json | jq .spec.ports[0].nodePort)
```
```
REGISTRY=127.0.0.1:$NODEPORT
```
```
kubectl get deploy -o json |
       jq ".items[] | {name:.metadata.name} + .spec.strategy.rollingUpdate"
```
Adding auto complete to your bash:
```
echo "source <(kubectl completion bash)" >> ~/.bashrc
```

## KOPS

Creating a K8s cluster with kops:

I have a user with S3/R53/EC2/VPC/IAM admin access and created a set of keys.

Create ssh key pairs with ssh-keygen and then set it:
```
kops create secret --name yoursubdomain.example.com sshpublickey admin -i ~/.ssh/yourkey.pub
```

1. Install Kops (you must have kubectl installed):
```
wget https://github.com/kubernetes/kops/releases/download/1.8.0/kops-linux-amd64
chmod +x kops-linux-amd64
mv kops-linux-amd64 /usr/local/bin/kops
```

2. Delegate authority for the subdomain you choose (get your subdomain nameservers and add a NS record to the parent hosted zone that contains these 4 ns records so it can know where to resolve your subdomain).

3. Create a S3 bucket and set the env var so Kops can manage files:
```
export KOPS_STATE_STORE=s3://your-bucket
```

4. Create cluster configuration (it's like a dry-run):
```
kops create cluster --node-count=2 --node-size=t2.small --node-volume-size=8 --master-volume-size=8 --zones=us-east-1a --name=yoursubdomain.example.com --master-size=t2.small --master-count=1 --dns-zone=yoursubdomain.example.com --ssh-public-key=~/.ssh/yourkey.pub
```
 
 #### Note:

I had an issue last week (mid sept. 2018) where the cluster failed, I don't know why (didn't have the time to get into that) I use t2.micro/nano because I'm just doing some tests, and after changing node/master size to t2.small, worked.
 
5. Create the cluster (it only adds --yes to the previous command (that's why I called it dry run)):
```
kops update cluster yoursubdomain.example.com --yes
```

Optional: Check every 5s the status of the cluster creation:
```
watch -n 5 kops validate cluster
```

#### Edit the cluster with Kops  (ig stands for instance group):
This will adjust the ig configuration but not the launch configuration at AWS:
```
kops edit ig 
```
Apply these changes on AWS:
```
kops update cluster --yes
```
After that kops rolling-update cluster should show you that updates could be performed.

Execute rolling-update with the --yes parameter and your master instance size should be changed:
```
kops rolling-update --yes
```
## HELM / Tiller

Install Helm (I'll create a script for this):
```
wget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz && 
tar -zxvf helm-v2.9.1-linux-amd64.tar.gz && mv linux-amd64/helm /usr/bin/
```

Create Tiller serviceaccount:
```
kubectl --namespace kube-system create serviceaccount tiller 
```

Bind the tiller serviceaccount to the Kubernetes cluster-admin role:
```
kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
```

Install Tiller in your Kubernetes cluster:
```
helm init --service-account tiller
```

Optional, run this and should Tiller's pod running at kube-system namespace:
```
kubectl get pods --namespace kube-system
```

In case of tiller issues: http://zero-to-jupyterhub.readthedocs.io/en/latest/setup-helm.html

Reference:

https://github.com/helm/helm/blob/master/docs/rbac.md

https://docs.gitlab.com/ee/install/kubernetes/preparation/tiller.html


### Further reading / notes:

- A service account provides an identity for processes that run in a Pod.
When you access the cluster (for example, using kubectl), you are authenticated by the API Server as a particular User Account (currently this is usually admin, unless your cluster administrator has customized your cluster). Processes in containers inside pods can also contact the apiserver. When they do, they are authenticated as a particular Service Account (for example, default)

Reference : https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/

### Additional commands:

SSH to cluster:

ssh -i ~/.ssh/yourkey admin@api.yoursubdomain.example.com
